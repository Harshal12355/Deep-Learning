{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing between batch norm and no batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CNN with and without batch normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CNN class without batch norm \n",
    "\n",
    "class CNNwithoutBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNwithoutBN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Convolutional Layer 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Convolutional Layer 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x) # Passing the input through the conv layers\n",
    "        x = x.view(x.size(0), -1) # Flattening the output of the conv layers\n",
    "        x = self.fc_layers(x) # Passing the flattened output through the fc layers\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CNN class with batch norm\n",
    "\n",
    "class CNNwithBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNwithBN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Convolutional Layer 1 \n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32), # Batch Normalization <-- Adding Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Convolutional Layer 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64), #Batch Normalization <-- Adding Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.BatchNorm1d(128), # Batch Normalization <-- Adding Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10) # Passing the flattened output through the fc layers\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining training and evaluating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0 # Initialize running loss\n",
    "        correct = 0 # Initialize number of correct predictions\n",
    "        total = 0 # Initialize total number of predictions\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad() # Zero the gradients \n",
    "            outputs = model(inputs) # Forward pass \n",
    "            loss = criterion(outputs, targets) # Calculate the loss\n",
    "            loss.backward() # Backward pass \n",
    "            optimizer.step() # Update the weights   \n",
    "\n",
    "            # Calculate metrics\n",
    "            running_loss += loss.item() # Add the loss to the running loss\n",
    "            _, predicted = torch.max(outputs, 1) # Get the predicted class \n",
    "            total += targets.size(0) # Add the number of predictions to the total\n",
    "            correct += (predicted == targets).sum().item() # Add the number of correct predictions to the total\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the hyperparameters\n",
    "\n",
    "model_without_bn = CNNwithoutBN()\n",
    "model_with_bn = CNNwithBN()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate at 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN without Batch Normalization:\n",
      "Epoch 1/10, Loss: 0.1943, Accuracy: 94.09%\n",
      "Epoch 2/10, Loss: 0.0503, Accuracy: 98.50%\n",
      "Epoch 3/10, Loss: 0.0361, Accuracy: 98.88%\n",
      "Epoch 4/10, Loss: 0.0266, Accuracy: 99.15%\n",
      "Epoch 5/10, Loss: 0.0202, Accuracy: 99.33%\n",
      "Epoch 6/10, Loss: 0.0164, Accuracy: 99.44%\n",
      "Epoch 7/10, Loss: 0.0126, Accuracy: 99.59%\n",
      "Epoch 8/10, Loss: 0.0101, Accuracy: 99.68%\n",
      "Epoch 9/10, Loss: 0.0094, Accuracy: 99.69%\n",
      "Epoch 10/10, Loss: 0.0078, Accuracy: 99.74%\n",
      "Finished Training\n",
      "Test Accuracy: 99.05%\n",
      "\n",
      "Training CNN with Batch Normalization:\n",
      "Epoch 1/10, Loss: 0.1121, Accuracy: 97.45%\n",
      "Epoch 2/10, Loss: 0.0333, Accuracy: 99.02%\n",
      "Epoch 3/10, Loss: 0.0219, Accuracy: 99.37%\n",
      "Epoch 4/10, Loss: 0.0129, Accuracy: 99.62%\n",
      "Epoch 5/10, Loss: 0.0121, Accuracy: 99.66%\n",
      "Epoch 6/10, Loss: 0.0087, Accuracy: 99.71%\n",
      "Epoch 7/10, Loss: 0.0056, Accuracy: 99.86%\n",
      "Epoch 8/10, Loss: 0.0058, Accuracy: 99.81%\n",
      "Epoch 9/10, Loss: 0.0052, Accuracy: 99.85%\n",
      "Epoch 10/10, Loss: 0.0059, Accuracy: 99.81%\n",
      "Finished Training\n",
      "Test Accuracy: 99.15%\n"
     ]
    }
   ],
   "source": [
    "optimizer_without_bn = optim.Adam(model_without_bn.parameters(), lr=0.001)\n",
    "optimizer_with_bn = optim.Adam(model_with_bn.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate CNN without Batch Normalization\n",
    "print(\"Training CNN without Batch Normalization:\")\n",
    "train_model(model_without_bn, train_loader, criterion, optimizer_without_bn)\n",
    "evaluate_model(model_without_bn, test_loader)\n",
    "\n",
    "# Train and evaluate CNN with Batch Normalization\n",
    "print(\"\\nTraining CNN with Batch Normalization:\")\n",
    "train_model(model_with_bn, train_loader, criterion, optimizer_with_bn)\n",
    "evaluate_model(model_with_bn, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate at 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN without Batch Normalization:\n",
      "Epoch 1/10, Loss: 0.0727, Accuracy: 97.86%\n",
      "Epoch 2/10, Loss: 0.0540, Accuracy: 98.39%\n",
      "Epoch 3/10, Loss: 0.0496, Accuracy: 98.57%\n",
      "Epoch 4/10, Loss: 0.0421, Accuracy: 98.86%\n",
      "Epoch 5/10, Loss: 0.0379, Accuracy: 98.92%\n",
      "Epoch 6/10, Loss: 0.0380, Accuracy: 98.99%\n",
      "Epoch 7/10, Loss: 0.0404, Accuracy: 98.90%\n",
      "Epoch 8/10, Loss: 0.0470, Accuracy: 98.86%\n",
      "Epoch 9/10, Loss: 0.0327, Accuracy: 99.17%\n",
      "Epoch 10/10, Loss: 0.0383, Accuracy: 99.14%\n",
      "Finished Training\n",
      "Test Accuracy: 98.48%\n",
      "\n",
      "Training CNN with Batch Normalization:\n",
      "Epoch 1/10, Loss: 0.0550, Accuracy: 98.28%\n",
      "Epoch 2/10, Loss: 0.0272, Accuracy: 99.11%\n",
      "Epoch 3/10, Loss: 0.0219, Accuracy: 99.30%\n",
      "Epoch 4/10, Loss: 0.0165, Accuracy: 99.45%\n",
      "Epoch 5/10, Loss: 0.0158, Accuracy: 99.48%\n",
      "Epoch 6/10, Loss: 0.0100, Accuracy: 99.67%\n",
      "Epoch 7/10, Loss: 0.0140, Accuracy: 99.52%\n",
      "Epoch 8/10, Loss: 0.0108, Accuracy: 99.66%\n",
      "Epoch 9/10, Loss: 0.0098, Accuracy: 99.67%\n",
      "Epoch 10/10, Loss: 0.0100, Accuracy: 99.69%\n",
      "Finished Training\n",
      "Test Accuracy: 98.97%\n"
     ]
    }
   ],
   "source": [
    "optimizer_without_bn = optim.Adam(model_without_bn.parameters(), lr=0.01)\n",
    "optimizer_with_bn = optim.Adam(model_with_bn.parameters(), lr=0.01)\n",
    "\n",
    "# Train and evaluate CNN without Batch Normalization\n",
    "print(\"Training CNN without Batch Normalization:\")\n",
    "train_model(model_without_bn, train_loader, criterion, optimizer_without_bn)\n",
    "evaluate_model(model_without_bn, test_loader)\n",
    "\n",
    "# Train and evaluate CNN with Batch Normalization\n",
    "print(\"\\nTraining CNN with Batch Normalization:\")\n",
    "train_model(model_with_bn, train_loader, criterion, optimizer_with_bn)\n",
    "evaluate_model(model_with_bn, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate at 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN without Batch Normalization:\n",
      "Epoch 1/10, Loss: 0.0190, Accuracy: 99.59%\n",
      "Epoch 2/10, Loss: 0.0122, Accuracy: 99.67%\n",
      "Epoch 3/10, Loss: 0.0119, Accuracy: 99.71%\n",
      "Epoch 4/10, Loss: 0.0105, Accuracy: 99.77%\n",
      "Epoch 5/10, Loss: 0.0141, Accuracy: 99.71%\n",
      "Epoch 6/10, Loss: 0.0046, Accuracy: 99.87%\n",
      "Epoch 7/10, Loss: 0.0070, Accuracy: 99.84%\n",
      "Epoch 8/10, Loss: 0.0142, Accuracy: 99.75%\n",
      "Epoch 9/10, Loss: 0.0109, Accuracy: 99.78%\n",
      "Epoch 10/10, Loss: 0.0113, Accuracy: 99.79%\n",
      "Finished Training\n",
      "Test Accuracy: 98.79%\n",
      "\n",
      "Training CNN with Batch Normalization:\n",
      "Epoch 1/10, Loss: 0.0046, Accuracy: 99.84%\n",
      "Epoch 2/10, Loss: 0.0026, Accuracy: 99.93%\n",
      "Epoch 3/10, Loss: 0.0026, Accuracy: 99.92%\n",
      "Epoch 4/10, Loss: 0.0030, Accuracy: 99.91%\n",
      "Epoch 5/10, Loss: 0.0014, Accuracy: 99.96%\n",
      "Epoch 6/10, Loss: 0.0032, Accuracy: 99.90%\n",
      "Epoch 7/10, Loss: 0.0027, Accuracy: 99.90%\n",
      "Epoch 8/10, Loss: 0.0022, Accuracy: 99.94%\n",
      "Epoch 9/10, Loss: 0.0022, Accuracy: 99.92%\n",
      "Epoch 10/10, Loss: 0.0020, Accuracy: 99.94%\n",
      "Finished Training\n",
      "Test Accuracy: 99.17%\n"
     ]
    }
   ],
   "source": [
    "optimizer_without_bn = optim.Adam(model_without_bn.parameters(), lr=0.005)\n",
    "optimizer_with_bn = optim.Adam(model_with_bn.parameters(), lr=0.005)\n",
    "\n",
    "# Train and evaluate CNN without Batch Normalization\n",
    "print(\"Training CNN without Batch Normalization:\")\n",
    "train_model(model_without_bn, train_loader, criterion, optimizer_without_bn)\n",
    "evaluate_model(model_without_bn, test_loader)\n",
    "\n",
    "# Train and evaluate CNN with Batch Normalization\n",
    "print(\"\\nTraining CNN with Batch Normalization:\")\n",
    "train_model(model_with_bn, train_loader, criterion, optimizer_with_bn)\n",
    "evaluate_model(model_with_bn, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Effects of Batch Normalization in Convolutional Neural Networks\n",
    "\n",
    "This experiment investigates the impact of Batch Normalization (BN) on:\n",
    "1. **Convergence Speed** of the network during training.\n",
    "2. **Permissible Learning Rates** for stable and effective training.\n",
    "3. **Overall Model Performance** in terms of accuracy and loss.\n",
    "\n",
    "## Experiment Setup\n",
    "\n",
    "### Datasets\n",
    "- **MNIST**: A dataset of 28x28 pixel grayscale images of handwritten digits, containing 60,000 training samples and 10,000 test samples.\n",
    "\n",
    "### Models\n",
    "1. **CNN without Batch Normalization**: Standard convolutional layers followed by activation and pooling layers.\n",
    "2. **CNN with Batch Normalization**: Same architecture but with Batch Normalization layers after each convolutional layer.\n",
    "\n",
    "### Training Parameters\n",
    "- **Epochs**: 10\n",
    "- **Optimizer**: Adam\n",
    "- **Learning Rates Tested**: 0.001, 0.005, 0.01\n",
    "\n",
    "## Results\n",
    "\n",
    "### Learning Rate = 0.001\n",
    "\n",
    "| Model                    | Epoch 1 Loss | Epoch 10 Loss | Epoch 1 Accuracy | Epoch 10 Accuracy | Test Accuracy |\n",
    "|--------------------------|--------------|---------------|------------------|--------------------|---------------|\n",
    "| **Without BN**           | 0.1943       | 0.0078       | 94.09%          | 99.74%            | 99.05%        |\n",
    "| **With BN**              | 0.1121       | 0.0059       | 97.45%          | 99.81%            | 99.15%        |\n",
    "\n",
    "**Observations**:\n",
    "- **Convergence Speed**: The model with BN converged faster, showing significant accuracy and loss improvement within the first few epochs.\n",
    "- **Performance**: With BN, the model achieved slightly higher test accuracy and lower final loss, indicating improved generalization.\n",
    "\n",
    "### Learning Rate = 0.005\n",
    "\n",
    "| Model                    | Epoch 1 Loss | Epoch 10 Loss | Epoch 1 Accuracy | Epoch 10 Accuracy | Test Accuracy |\n",
    "|--------------------------|--------------|---------------|------------------|--------------------|---------------|\n",
    "| **Without BN**           | 0.0190       | 0.0113       | 99.59%          | 99.79%            | 98.79%        |\n",
    "| **With BN**              | 0.0046       | 0.0020       | 99.84%          | 99.94%            | 99.17%        |\n",
    "\n",
    "**Observations**:\n",
    "- **Learning Rate Stability**: BN allowed the model to handle the larger learning rate without significant instability, unlike the model without BN, which had noticeable fluctuations.\n",
    "- **Performance**: The model with BN continued to perform better in test accuracy and maintained lower final training loss.\n",
    "\n",
    "### Learning Rate = 0.01\n",
    "\n",
    "| Model                    | Epoch 1 Loss | Epoch 10 Loss | Epoch 1 Accuracy | Epoch 10 Accuracy | Test Accuracy |\n",
    "|--------------------------|--------------|---------------|------------------|--------------------|---------------|\n",
    "| **Without BN**           | 0.0727       | 0.0383       | 97.86%          | 99.14%            | 98.48%        |\n",
    "| **With BN**              | 0.0550       | 0.0100       | 98.28%          | 99.69%            | 98.97%        |\n",
    "\n",
    "**Observations**:\n",
    "- **Convergence and Stability**: Without BN, the model showed slower convergence and higher loss at the larger learning rate, indicating training instability. With BN, the model trained more smoothly and with a higher final accuracy.\n",
    "- **Performance**: The BN model achieved better performance, although the difference was less pronounced than with lower learning rates.\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "1. **Accelerated Convergence**:\n",
    "   - Batch Normalization accelerates convergence across all learning rates, showing faster loss reduction and reaching higher accuracy within fewer epochs.\n",
    "\n",
    "2. **Larger Learning Rates**:\n",
    "   - The presence of BN permitted the use of larger learning rates (e.g., 0.005 and 0.01) without significant instability. This is likely due to BN's effect of reducing internal covariate shifts, making gradient updates more stable.\n",
    "\n",
    "3. **Improved Model Performance**:\n",
    "   - Models trained with BN achieved slightly better final accuracy and generalization across all learning rates, suggesting that BN may add a regularization effect that enhances performance.\n",
    "\n",
    "## Conclusion\n",
    "Batch Normalization is an effective technique for:\n",
    "- **Accelerating convergence** during training by normalizing activations.\n",
    "- **Enabling the use of larger learning rates** without destabilizing training.\n",
    "- **Improving model performance** with slight accuracy and loss improvements on test data.\n",
    "\n",
    "In summary, Batch Normalization is a valuable addition to neural networks, especially when training with higher learning rates or when convergence speed is critical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
